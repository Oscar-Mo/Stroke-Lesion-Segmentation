{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oscar-Mo/big_brain_BC/blob/main/training_(2%2B1)D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3lz4msXvvPp"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "In this notebook we train the UNet model with the modified (2+1)D DoubleConv class. The training set consists of 525 samples, representing 80% of the full dataset. The other 20% is dedicated to testing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up"
      ],
      "metadata": {
        "id": "IMXVUZ51jd-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWeuApWsSTj_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrcw02cdYF6X"
      },
      "outputs": [],
      "source": [
        "!pip install bids\n",
        "!pip install git+https://github.com/npnl/bidsio\n",
        "import bidsio\n",
        "bids_loader = bidsio.BIDSLoader(data_entities=[{'subject': '',\n",
        "                                               'session': '',\n",
        "                                               'suffix': 'T1w',\n",
        "                                               'space': 'MNI152NLin2009aSym'}],\n",
        "                                target_entities=[{'suffix': 'mask',\n",
        "                                                'label': 'L',\n",
        "                                                'desc': 'T1lesion'}],\n",
        "                                data_derivatives_names=['ATLAS'],\n",
        "                                target_derivatives_names=['ATLAS'],\n",
        "                                batch_size=2,\n",
        "                                root_dir='drive/MyDrive/big_brain/split1/train/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2QsKN0hYmNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcf8a9f-95b8-4a3e-c2a6-fbef8ed85d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 525 subjects in our dataset.\n",
            "Every sample loads 2 images.\n",
            "Images have these dimensions: (197, 233, 189)\n",
            "Every batch will load 2 samples.\n"
          ]
        }
      ],
      "source": [
        "tmp = bids_loader.load_sample(0)\n",
        "print(f'There are {len(bids_loader)} subjects in our dataset.')\n",
        "print(f'Every sample loads {len(tmp)} images.')\n",
        "print(f'Images have these dimensions: {bids_loader.data_shape}')\n",
        "print(f'Every batch will load {bids_loader.batch_size} samples.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "s7Rgzoh7nJNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from UNet_model_2_plus_1D import UNet\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = UNet(n_channels=1, n_classes=1)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "_MCQdbbAThpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()                            \n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "        \n",
        "        return Dice_BCE\n",
        "\n",
        "loss_func = DiceBCELoss()"
      ],
      "metadata": {
        "id": "sRKRgnHwOIni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW95jjqlkTJM"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def downsize_64(batch):\n",
        "  if (len(batch.shape) == 4):\n",
        "    batch = batch.unsqueeze(0)\n",
        "  num_samples = batch.shape[0]\n",
        "  scale = 3\n",
        "  batch = batch[:,:,::scale,::scale,::scale]\n",
        "  batch = batch[:,:,:64,:64]\n",
        "  zeros = torch.zeros(num_samples,1,64,64,1)\n",
        "  batch = torch.cat((zeros, batch), 4)\n",
        "  return batch\n",
        "\n",
        "def downsize_128(batch):\n",
        "  num_samples = batch.shape[0]\n",
        "  d = torch.linspace(-1, 1, 128)\n",
        "  meshz, meshy, meshx = torch.meshgrid((d, d, d))\n",
        "  grid = torch.stack((meshx, meshy, meshz), 3)\n",
        "  grid = grid.unsqueeze(0)\n",
        "  grid = grid.repeat_interleave(num_samples, dim=0)\n",
        "\n",
        "  return F.grid_sample(batch, grid, align_corners=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOEUt1iGNMS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e26dc8-a6b4-4ed2-d419-5fd0919c0427"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------[1,    10] loss: 1.48061694\n",
            "----------[1,    20] loss: 1.40900997\n",
            "----------[1,    30] loss: 1.34488068\n",
            "----------[1,    40] loss: 1.29072158\n",
            "----------[1,    50] loss: 1.26043851\n",
            "----------[1,    60] loss: 1.23049161\n",
            "----------[1,    70] loss: 1.20349872\n",
            "----------[1,    80] loss: 1.18199406\n",
            "----------[1,    90] loss: 1.16449884\n",
            "----------[1,   100] loss: 1.15069263\n",
            "----------[1,   110] loss: 1.13675107\n",
            "----------[1,   120] loss: 1.12389805\n",
            "----------[1,   130] loss: 1.10656959\n",
            "----------[1,   140] loss: 1.10770390\n",
            "----------[1,   150] loss: 1.10486622\n",
            "----------[1,   160] loss: 1.09663471\n",
            "----------[1,   170] loss: 1.08299068\n",
            "----------[1,   180] loss: 1.08667674\n",
            "----------[1,   190] loss: 1.07984316\n",
            "----------[1,   200] loss: 1.07051530\n",
            "----------[1,   210] loss: 1.06200569\n",
            "----------[1,   220] loss: 1.04223539\n",
            "----------[1,   230] loss: 1.04669147\n",
            "----------[1,   240] loss: 1.03013310\n",
            "----------[1,   250] loss: 1.04143755\n",
            "----------[1,   260] loss: 1.05720116\n",
            "----------[2,   270] loss: 0.72795802\n",
            "----------[2,   280] loss: 1.04638083\n",
            "----------[2,   290] loss: 1.00560801\n",
            "----------[2,   300] loss: 0.99179161\n",
            "----------[2,   310] loss: 0.97999315\n",
            "----------[2,   320] loss: 1.04362715\n",
            "----------[2,   330] loss: 1.03732108\n",
            "----------[2,   340] loss: 1.03430501\n",
            "----------[2,   350] loss: 1.02903048\n",
            "----------[2,   360] loss: 1.03588113\n",
            "----------[2,   370] loss: 1.03336775\n",
            "----------[2,   380] loss: 1.01816081\n",
            "----------[2,   390] loss: 0.98370423\n",
            "----------[2,   400] loss: 0.96313511\n",
            "----------[2,   410] loss: 0.99218341\n",
            "----------[2,   420] loss: 0.96866027\n",
            "----------[2,   430] loss: 0.96885920\n",
            "----------[2,   440] loss: 0.98642256\n",
            "----------[2,   450] loss: 0.99893868\n",
            "----------[2,   460] loss: 0.96266943\n",
            "----------[2,   470] loss: 0.97285814\n",
            "----------[2,   480] loss: 0.85715024\n",
            "----------[2,   490] loss: 0.85787596\n",
            "----------[2,   500] loss: 0.82021842\n",
            "----------[2,   510] loss: 0.81250756\n",
            "----------[2,   520] loss: 1.01862863\n",
            "----------[3,   530] loss: 0.37775970\n",
            "----------[3,   540] loss: 0.92456805\n",
            "----------[3,   550] loss: 0.81111031\n",
            "----------[3,   560] loss: 0.79356014\n",
            "----------[3,   570] loss: 0.64978369\n",
            "----------[3,   580] loss: 0.99875467\n",
            "----------[3,   590] loss: 0.99952067\n",
            "----------[3,   600] loss: 0.99874594\n",
            "----------[3,   610] loss: 0.94520592\n",
            "----------[3,   620] loss: 0.98900609\n",
            "----------[3,   630] loss: 0.98832465\n",
            "----------[3,   640] loss: 0.99251039\n",
            "----------[3,   650] loss: 0.76337756\n",
            "----------[3,   660] loss: 0.79301549\n",
            "----------[3,   670] loss: 0.78983675\n",
            "----------[3,   680] loss: 0.81473853\n",
            "----------[3,   690] loss: 0.79638339\n",
            "----------[3,   700] loss: 0.80114866\n",
            "----------[3,   710] loss: 0.88438754\n",
            "----------[3,   720] loss: 0.78083744\n",
            "----------[3,   730] loss: 0.79095941\n",
            "----------[3,   740] loss: 0.85859003\n",
            "----------[3,   750] loss: 0.65943781\n",
            "----------[3,   760] loss: 0.66671001\n",
            "----------[3,   770] loss: 0.56201885\n",
            "----------[3,   780] loss: 0.95697375\n",
            "----------[4,   790] loss: 0.06475705\n",
            "----------[4,   800] loss: 0.84295911\n",
            "----------[4,   810] loss: 0.68811330\n",
            "----------[4,   820] loss: 0.63307548\n",
            "----------[4,   830] loss: 0.43125176\n",
            "----------[4,   840] loss: 0.78168154\n",
            "----------[4,   850] loss: 1.02111409\n",
            "----------[4,   860] loss: 0.99436239\n",
            "----------[4,   870] loss: 0.93940628\n",
            "----------[4,   880] loss: 0.98202137\n",
            "----------[4,   890] loss: 0.96413071\n",
            "----------[4,   900] loss: 0.95573865\n",
            "----------[4,   910] loss: 0.88095849\n",
            "----------[4,   920] loss: 0.67550867\n",
            "----------[4,   930] loss: 0.71064087\n",
            "----------[4,   940] loss: 0.81163318\n",
            "----------[4,   950] loss: 0.85662118\n",
            "----------[4,   960] loss: 0.73865635\n",
            "----------[4,   970] loss: 0.88117069\n",
            "----------[4,   980] loss: 0.87866721\n",
            "----------[4,   990] loss: 0.72415574\n",
            "----------[4,  1000] loss: 0.74456968\n",
            "----------[4,  1010] loss: 0.57509074\n",
            "----------[4,  1020] loss: 0.70571021\n",
            "----------[4,  1030] loss: 0.48093930\n",
            "----------[4,  1040] loss: 0.73802482\n",
            "----------[4,  1050] loss: 0.90546302\n",
            "----------[5,  1060] loss: 0.52918140\n",
            "----------[5,  1070] loss: 0.73616495\n",
            "----------[5,  1080] loss: 0.51417059\n",
            "----------[5,  1090] loss: 0.39536842\n",
            "----------[5,  1100] loss: 0.57201532\n",
            "----------[5,  1110] loss: 0.92252909\n",
            "----------[5,  1120] loss: 0.91948429\n",
            "----------[5,  1130] loss: 0.89549905\n",
            "----------[5,  1140] loss: 0.82598012\n",
            "----------[5,  1150] loss: 0.90583960\n",
            "----------[5,  1160] loss: 0.88826751\n",
            "----------[5,  1170] loss: 0.77260396\n",
            "----------[5,  1180] loss: 0.57992560\n",
            "----------[5,  1190] loss: 0.55791711\n",
            "----------[5,  1200] loss: 0.65458909\n",
            "----------[5,  1210] loss: 0.67296784\n",
            "----------[5,  1220] loss: 0.56550442\n",
            "----------[5,  1230] loss: 0.71620007\n",
            "----------[5,  1240] loss: 0.69278271\n",
            "----------[5,  1250] loss: 0.69267942\n",
            "----------[5,  1260] loss: 0.60436252\n",
            "----------[5,  1270] loss: 0.74387684\n",
            "----------[5,  1280] loss: 0.50797816\n",
            "----------[5,  1290] loss: 0.52488993\n",
            "----------[5,  1300] loss: 0.53117835\n",
            "----------[5,  1310] loss: 0.93272151\n",
            "----------[6,  1320] loss: 0.29005859\n",
            "----------[6,  1330] loss: 0.66393930\n",
            "----------[6,  1340] loss: 0.40685236\n",
            "----------[6,  1350] loss: 0.44673092\n",
            "----------[6,  1360] loss: 0.39965255\n",
            "----------[6,  1370] loss: 0.88299003\n",
            "----------[6,  1380] loss: 0.88416398\n",
            "----------[6,  1390] loss: 0.93949584\n",
            "----------[6,  1400] loss: 0.78667656\n",
            "----------[6,  1410] loss: 0.88027393\n",
            "----------[6,  1420] loss: 0.86387584\n",
            "----------[6,  1430] loss: 0.89845913\n",
            "----------[6,  1440] loss: 0.46576959\n",
            "----------[6,  1450] loss: 0.52970007\n",
            "----------[6,  1460] loss: 0.50884273\n",
            "----------[6,  1470] loss: 0.52905891\n",
            "----------[6,  1480] loss: 0.54837956\n",
            "----------[6,  1490] loss: 0.50768972\n",
            "----------[6,  1500] loss: 0.75915773\n",
            "----------[6,  1510] loss: 0.53135450\n",
            "----------[6,  1520] loss: 0.57010497\n",
            "----------[6,  1530] loss: 0.56797193\n",
            "----------[6,  1540] loss: 0.37394313\n",
            "----------[6,  1550] loss: 0.50237758\n",
            "----------[6,  1560] loss: 0.37250547\n",
            "----------[6,  1570] loss: 0.90879854\n",
            "----------[7,  1580] loss: 0.08940412\n",
            "----------[7,  1590] loss: 0.54257751\n",
            "----------[7,  1600] loss: 0.46805560\n",
            "----------[7,  1610] loss: 0.37606857\n",
            "----------[7,  1620] loss: 0.30202155\n",
            "----------[7,  1630] loss: 0.68200064\n",
            "----------[7,  1640] loss: 0.92864585\n",
            "----------[7,  1650] loss: 0.82551198\n",
            "----------[7,  1660] loss: 0.76719770\n",
            "----------[7,  1670] loss: 0.82275114\n",
            "----------[7,  1680] loss: 0.75337915\n",
            "----------[7,  1690] loss: 0.72502772\n",
            "----------[7,  1700] loss: 0.55706856\n",
            "----------[7,  1710] loss: 0.49332936\n",
            "----------[7,  1720] loss: 0.39539051\n",
            "----------[7,  1730] loss: 0.49330280\n",
            "----------[7,  1740] loss: 0.48393407\n",
            "----------[7,  1750] loss: 0.46327467\n",
            "----------[7,  1760] loss: 0.65318496\n",
            "----------[7,  1770] loss: 0.57798877\n",
            "----------[7,  1780] loss: 0.54280455\n",
            "----------[7,  1790] loss: 0.57808331\n",
            "----------[7,  1800] loss: 0.34631834\n",
            "----------[7,  1810] loss: 0.49704711\n",
            "----------[7,  1820] loss: 0.37832683\n",
            "----------[7,  1830] loss: 0.69620316\n",
            "----------[7,  1840] loss: 0.79597812\n",
            "-"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for epoch in range(7):\n",
        "    running_loss = 0.0\n",
        "    for data, label in bids_loader.load_batches():\n",
        "\n",
        "        data = torch.Tensor(data)\n",
        "        label = torch.Tensor(label)\n",
        "\n",
        "        data = downsize_128(data).to(device)\n",
        "        label = downsize_128(label).to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(data)\n",
        "        loss = loss_func(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        print('-', end =\"\")\n",
        "        if (i % 10 == 9):\n",
        "          loss_str = f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10 :.8f}'\n",
        "          print(loss_str)\n",
        "          running_loss = 0.0\n",
        "\n",
        "          f = open(\"drive/MyDrive/big_brain/models/split1/loss.txt\", \"a\")\n",
        "          f.write(loss_str + '\\n')\n",
        "          f.close()\n",
        "\n",
        "          torch.save(model.state_dict(), 'drive/MyDrive/big_brain/models/'+model_name+'/model_weights_'+model_name+'.pth')\n",
        "        i += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hkP8THPrmJaRlm1QVfXGkZn-Uz6x3z_X",
      "authorship_tag": "ABX9TyO8XX4MfuVaP+XpY8u8P84r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}